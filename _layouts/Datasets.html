<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
{% include _head.html %}
<style>
a:link {
    color: blue;
    background-color: transparent;
    text-decoration: none;
}
a:visited {
    color: blue;
    background-color: transparent;
    text-decoration: none;
}
a:hover {
    color: red;
    background-color: transparent;
    text-decoration: underline;
}
a:active {
    color: yellow;
    background-color: transparent;
    text-decoration: underline;
}
</style>
</head>

<body class="page">
<style>
    /*********************************
     The list of publication items
     *********************************/
/* The list of items */
.biblist { }

/* The item */
.biblist li { }

/* You can define custom styles for plstyle field here. */


/*************************************
 The box that contain BibTeX code
 *************************************/
div.noshow { display: none; }
div.bibtex {
	margin-right: 0%;
	margin-top: 1.2em;
	margin-bottom: 1em;
	border: 1px solid silver;
	padding: 0em 1em;
	background: #ffffee;
}
div.bibtex pre { font-size: 75%; overflow: auto;  width: 100%; padding: 0em 0em;}</style>
<script type="text/javascript">
    <!--
    // Toggle Display of BibTeX
    function toggleBibtex(articleid) {
        var bib = document.getElementById('bib_'+articleid);
        if (bib) {
            if(bib.className.indexOf('bibtex') != -1) {
                bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex';
            }
        } else {
            return;
        }
    }
-->
    </script>

{% include _browser-upgrade.html %}

{% include _navigation.html %}

{% if page.image.feature %}
  <div class="image-wrap">
  <img src=
    {% if page.image.feature contains 'http://' %}
      "{{ page.image.feature }}"
    {% elsif page.image.feature contains 'https://' %}
      "{{ page.image.feature }}"
    {% else %}
      "{{ site.url }}/images/{{ page.image.feature }}"
    {% endif %}
  alt="{{ page.title }} feature image">
  {% if page.image.credit %}
    <span class="image-credit">Photo Credit: <a href="{{ page.image.creditlink }}">{{ page.image.credit }}</a></span>
  {% endif %}
  </div><!-- /.image-wrap -->
{% endif %}

<div id="main" role="main">
  <div class="article-author-side">
    {% include _author-bio.html %}
  </div>
  <article>
    <h1>{{ page.title }}</h1>
    <div class="article-wrap">
      {{ content }}

<!----------------------------------------------------------------------------------->
<!-- Generated from JabRef by PubList by Truong Nghiem at 12:49 on 2016.07.15. -->

<!-------------------------------------------------------------------------------------------->

<!-- Item: rahman2024defan>	    
<li > <strong><a href="https://ieee-dataport.org/documents/sequential-storytelling-image-dataset-ssid"> SSID: </a></strong> Sequential Storytelling Image Dataset <br>
<strong> Description: </strong> &ldquo;DefAn is a comprehensive evaluation benchmark dataset, with more than 75000 samples, designed to assess the hallucination tendencies of large language models (LLMs). The dataset is categorized into eight knowledge domains: Sports, Census Australia, Nobel Prize, Entertainment, World Organizations, QS Ranking, Conference Venue, and Math. The dataset is structured into two parts: public and hidden.&rdquo;<br />
<a href="https://huggingface.co/datasets/iamasQ/DefAn">[Dataset]</a>
<a href="javascript:toggleBibtex('rahman2024defan')">[BibTeX]</a>	
<a href="https://arxiv.org/pdf/2406.09155">[PDF]</a>
</p>
<div id="bib_rahman2024defan" class="bibtex noshow">
<pre>
@article{rahman2024defan,
      title={DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation}, 
      author={A B M Ashikur Rahman and Saeed Anwar and Muhammad Usman and Ajmal Mian},
      year={2024},
      eprint={2406.09155},
      archivePrefix={arXiv},
}
</pre>
</div>
</li>
	    
<!-- Item: Malakan2023SSID-->	    
<li > <strong><a href="https://ieee-dataport.org/documents/sequential-storytelling-image-dataset-ssid"> SSID: </a></strong> Sequential Storytelling Image Dataset <br>
<strong> Description: </strong> &ldquo;Visual Storytelling Task (VST) takes a set of images as input and aims to generate a coherent story relevant to the input images. We collected and manually annotated images from publicly available documentaries, lifestyles, and movies. In summary, the SSID dataset has 17,365 images of 3,473 unique sets of five images. Each set of images is associated with four ground truths, resulting in 13,892 unique ground truths (i.e., written stories). Each ground truth is composed of five connected sentences written as a story.&rdquo;<br />
<a href="https://ieee-dataport.org/documents/sequential-storytelling-image-dataset-ssid">[Dataset]</a>
<a href="javascript:toggleBibtex('Malakan2023SSID')">[BibTeX]</a>	
<a href="https://ieeexplore.ieee.org/document/10177150">[PDF]</a>
</p>
<div id="bib_Malakan2023SSID" class="bibtex noshow">
<pre>
@article{Malakan2023SSID,
 author={Malakan, Zainy M. and Anwar, Saeed and Hassan, Ghulam Mubashar and Mian, Ajmal},
  journal={IEEE Access}, 
  title={Sequential Vision to Language as Story: A Storytelling Dataset and Benchmarking}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/ACCESS.2023.3293646}
}
</pre>
</div>
</li>
	    
<!-- Item: Ibrahim2023SWAN-->	    
<li > <strong><a href="https://ieee-dataport.org/documents/swan-0"> Swan: </a></strong> A Point Cloud Dataset <br>
<strong> Description: </strong> &ldquo;A large-scale outdoor point cloud dataset captured with LiDAR sensor and annotated manually for semantic segmentation, instance segmentation and object detection.&rdquo;<br />
<a href="https://drive.google.com/drive/folders/11bcx8CizpAFohJy6HS2g032bbzVzGN7a?usp=sharing">[Dataset]</a>
<a href="javascript:toggleBibtex('Ibrahim2023SWAN')">[BibTeX]</a>	
<a href="https://ieeexplore.ieee.org/document/10046141">[PDF]</a>
</p>
<div id="bib_Ibrahim2023SWAN" class="bibtex noshow">
<pre>
@article{Ibrahim2023SWAN,
  author={Ibrahim, Muhammad and Akhtar, Naveed and Anwar, Saeed and Mian, Ajmal},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={SAT3D: Slot Attention Transformer for 3D Point Cloud Semantic Segmentation}, 
  year={2023},
  pages={1-11},
}
</pre>
</div>
</li>

<!-- Item: Ibrahim2023Slice-->	    
<li > <strong><a href="https://dx.doi.org/10.21227/s2p2-2e66"> Perth-WA Localization: </a></strong>A Point Cloud Dataset <br>
<strong> Description: </strong> &ldquo;LiDAR 3D point cloud map of Perth, Western Australia is provided with ground truth locations and orientations.&rdquo;<br />
<a href="https://drive.google.com/drive/folders/11bcx8CizpAFohJy6HS2g032bbzVzGN7a?usp=sharing">[Dataset]</a>
<a href="javascript:toggleBibtex('Ibrahim2023Slice')">[BibTeX]</a>	
<a href="https://arxiv.org/abs/2301.08957">[PDF]</a>
</p>
<div id="bib_Ibrahim2023Slice" class="bibtex noshow">
<pre>
@article{Ibrahim2023slice,
  title={Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps},
  author={Ibrahim, Muhammad and Akhtar, Naveed and Anwar, Saeed and Wise, Michael and Mian, Ajmal},
  journal={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2023}
}
</pre>
</div>
</li>
	
<!-- Item: Anwar2022FGBR-->	    
<li > <strong><a href="https://drive.google.com/drive/folders/11bcx8CizpAFohJy6HS2g032bbzVzGN7a?usp=sharing"> FGBR: </a></strong> Fine Grained Butterflies and Reef Fish Dataset <br>
<strong> Description: </strong> &ldquo;FGBR contains two species, butterfly and reef fish. The butterfly dataset has 50 species, consisting of 2613 images, most of which are ecological. The reef fish also has 50 species. Most of the 3825 images in the reef fish dataset are ecological.&rdquo;<br />
<a href="https://drive.google.com/drive/folders/11bcx8CizpAFohJy6HS2g032bbzVzGN7a?usp=sharing">[Dataset]</a>
<a href="javascript:toggleBibtex('Anwar2022FGBR')">[BibTeX]</a>	
<a href="https://www.mdpi.com/2079-9292/11/17/2701">[PDF]</a>
</p>
<div id="bib_Anwar2022FGBR" class="bibtex noshow">
<pre>
@article{Anwar2022FGBR,
  title={Towards Low-Cost Classification for Novel Fine-Grained Datasets},
  author={Anwar, Abbas and Anwar, Hafeez and Anwar, Saeed},
  journal={Electronics},
  year={2022}
}
</pre>
</div>
</li>
	
<!-- Item: Qin2022ANUBIS -->	    
<li > <strong><a href="http://hcc-workshop.anu.edu.au/webs/anu101/home"> ANUBIS: </a></strong> Australian National University Benchmarking Indoor Skeleton <br>
<strong> Description: </strong> &ldquo;ANUBIS is a large-scale human skeleton dataset containing 80 actions. Compared with previously collected datasets, ANUBIS is advantageous in the following four aspects: (1) employing more recently released sensors; (2) containing novel back view; (3) encouraging high enthusiasm of subjects; (4) including actions of the COVID pandemic era.&rdquo;<br />
<a href="http://hcc-workshop.anu.edu.au/webs/anu101/home">[Dataset]</a>
<a href="javascript:toggleBibtex('Qin2022ANUBIS')">[BibTeX]</a>	
<a href="https://arxiv.org/abs/2205.02071">[PDF]</a>
</p>
<div id="bib_Qin2022ANUBIS" class="bibtex noshow">
<pre>
@article{Qin2022ANUBIS,
  title={ANUBIS: Skeleton Action Recognition Dataset, Review, and Benchmark},
  author={Qin, Zhenyue and Liu, Yang and Perera, Madhawa and Gedeon, Tom and Ji, Pan and Kim, Dongwoo and Anwar, Saeed},
  journal={arXiv preprint arXiv:2205.02071},
  year={2022}
}
</pre>
</div>
</li>
	    
	      
	    
<!-- Item: Usama2022VTLPR -->
<li > <strong> <a href="https://github.com/usama-x930/VT-LPR"> DVLPD: </a></strong> Diverse Vehicle and License Plates Dataset<br>
<strong> Description: </strong> &ldquo;We collect a novel dataset that has 10k images of Pakistani vehicles. These vehicles are trucks, buses, vans, carry vans, and cars. There are two sub-classes of trucks called type1 trucks having a single axle and type2 trucks with double axles. After dataset collection, the pre-processing steps and ground truth generation are performed to train and test the CNN models.&rdquo;<br />
<a href="https://github.com/usama-x930/VT-LPR">[Dataset]</a>
<a href="javascript:toggleBibtex('Usama2022VTLPR')">[BibTeX]</a>	
<a href="https://arxiv.org/pdf/2202.05631.pdf">[PDF]</a>
</p>
<div id="bib_Usama2022VTLPR" class="bibtex noshow">
<pre>
@InProceedings{Usama2022VTLPR,
      title={Vehicle and License Plate Recognition with Novel Dataset for Toll Collection}, 
      author={Muhammad Usama and Hafeez Anwar and Muhammad Muaz Shahid and Abbas Anwar and Saeed Anwar and Helmuth Hlavacs},
      year={2022},
      eprint={2202.05631},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}
</pre>
</div>
</li>  
	    

<!-- Item: Han2021CWR -->
<li > <strong> <a href="https://data.csiro.au/collections/collection/CIcsiro:49488"> HICRD: </a></strong> Heron Island Coral Reef Dataset <br>
<strong> Description: </strong> &ldquo;Heron Island Coral Reef Dataset (HICRD) contains 6003 low-quality images, 3673 good-quality images, and 2000 restored images. We use low-quality images and restored images as the unpaired training set (trainA + trainB). In contrast, the paired training set contains good-quality (trainA_paired) images and corresponding restored images (trainB_paired). The test set contains 300 good-quality images (testA) as well as 300 paired restored images (testB) as ground truth. All images are in 1842 x 980 resolution. The copyright belongs to CSIRO (Commonwealth Scientific and Industrial Research Organisation).&rdquo;<br />
<a href="https://data.csiro.au/collections/collection/CIcsiro:49488">[Dataset]</a>
<a href="javascript:toggleBibtex('Han2021CWR')">[BibTeX]</a>
<!-- <a href="">[PDF]</a> -->
<a href="https://arxiv.org/pdf/2106.10718.pdf">[arXiv]</a>

</p>
<div id="bib_Han2021CWR" class="bibtex noshow">
<pre>
@article{Han2021CWR,
      title={Underwater Image Restoration via Contrastive Learning and a Real-world Dataset}, 
      author={Junlin Han and Mehrdad Shoeiby and Tim Malthus and Elizabeth Botha and Janet Anstee and Saeed Anwar and Ran Wei and Mohammad Ali Armin and Hongdong Li and Lars Petersson},
      year={2021},
      eprint={2106.10718},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}
</pre>
</div>
</li>
	    	
<!-- Item:Anwar2021CoinNet -->
<li > <strong> <a href="https://drive.google.com/open?id=1Bxcoesctd4xKvXMdgbLTCLjQkC7peQPY"> RRCD: </a></strong> Roman Republican Coin Dataset <br>
<strong> Description: </strong> &ldquo;Based on Crawfordâ€™s work, we collect the most diverse and extensive image dataset of the reverse sides. For most of the Roman Republic coin classes, the ob- verse side depicts more discriminative information than the observe side. Our dataset has 228 motif classes, including 100 classes that are the main classes for training and testing, which we call the main dataset RRCD-Main. The images of the additional 128 classes constitute the disjoint test set, RRCD-Disjoint, which we allocate to assess the generalization ability of our models.,&rdquo;<br />
<a href="https://drive.google.com/open?id=1Bxcoesctd4xKvXMdgbLTCLjQkC7peQPY">[Dataset]</a>
<a href="javascript:toggleBibtex('Anwar2021CoinNet')">[BibTeX]</a>
<a href="https://www.sciencedirect.com/science/article/pii/S0031320321000583">[PDF]</a>
<a href="https://arxiv.org/pdf/1908.09428.pdf">[arXiv]</a>

</p>
<div id="bib_Anwar2021CoinNet" class="bibtex noshow">
<pre>
@article{Anwar2021CoinNet,
title = {Deep Ancient Roman Republican Coin Classification via Feature Fusion and Attention},
journal = {Pattern Recognition},
pages = {107871},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107871},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321000583},
author = {Hafeez Anwar and Saeed Anwar and Sebastian Zambanini and Fatih Porikli},
}
</pre>
</div>
</li>	
	

<!-- Item: Anwar2020ColorSurvey -->
<li > <strong> <a href="https://drive.google.com/file/d/1GpmEVNFn12bK0EoXK46FP3cXFUosomaG/view?usp=sharing"> NCD: </a></strong> Natural-Color Dataset<br>
<strong> Description: </strong> &ldquo;The Natural-Color Dataset (NCD) is an image colorization dataset where images are true to their colors. For example, a carrot will have an orange color in most images. Bananas will be either greenish or yellowish. It contains 723 images from the internet distributed in 20 categories. Each image has an object and a white background.&rdquo;<br />
<a href="https://drive.google.com/file/d/1GpmEVNFn12bK0EoXK46FP3cXFUosomaG/view?usp=sharing">[Dataset (GrayScale Images)]</a>
<a href="https://drive.google.com/file/d/1k_UvYzdrHbphW4UcbDb9jWB0ZQIAGEAo/view?usp=sharing">[Dataset (Color GroundTruth)]</a>
<a href="javascript:toggleBibtex('Anwar2020ColorSurvey')">[BibTeX]</a>
<a href="https://arxiv.org/pdf/2008.10774.pdf">[PDF]</a>

</p>
<div id="bib_Anwar2020ColorSurvey" class="bibtex noshow">
<pre>
@article{Anwar2020ColorSurvey,
  title={Image Colorization: A Survey and Dataset},
  author={Anwar, Saeed and Tahir, Muhammad and Li, Chongyi and Mian, Ajmal and Khan, Fahad Shahbaz and Muzaffar, Abdul Wahab},
  journal={arXiv preprint arXiv:2008.10774},
  year={2020}
}
</pre>
</div>
</li>
	    
	
<!-- Item: Anwar2019UWE -->
<li > <strong> <a href="https://github.com/saeed-anwar/UWCNN#datasets"> UWSD: </a></strong> UnderWater Synthetic Dataset <br>
<strong> Description: </strong> &ldquo;To synthesize underwater image degradation datasets, we use the attenuation coefficients described in Table 1 for the different water types of oceanic and coastal classes (i.e., I, IA, IB, II, and III for open ocean waters, and 1, 3, 5, 7, and 9 for coastal waters). Type-I is the clearest and Type-III is the most turbid open ocean water. Similarly, for coastal waters, Type-1 is the clearest and Type-9 is the most turbid. We apply Eqs (1) and (2) (please check the paper) to build ten types of underwater image datasets by using the RGB-D NYU-v2 indoor dataset which consists of 1449 images. To improve the quality of datasets, we crop the original size (480x640) of NYU-v2 to 460x620. This dataset is for non-commercial use only. The size of each dataset is 1.2GB&rdquo;<br />
<a href="https://github.com/saeed-anwar/UWCNN#datasets">[Dataset]</a>
<a href="javascript:toggleBibtex('Anwar2019UWE')">[BibTeX]</a>	
<a href="https://www.sciencedirect.com/science/article/pii/S0031320319303401#fig0010">[PR Version]</a>
<a href="https://arxiv.org/pdf/1807.03528.pdf">[PDF]</a>

</p>
<div id="bib_Anwar2019UWE" class="bibtex noshow">
<pre>
@article{Anwar2019UWE,
  title={Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement},
  author={Li, Chongyi and Anwar, Saeed},
  journal={Pattern Recognition},
  pages={107038},
  year={2019},
  publisher={Elsevier}
}
</pre>
</div>
</li>	

	    
<!-- Item: Anwar2021DefocusMVA -->
<li > <strong> <a href="../codes/DefocusingImagesCode.zip"> SMD: </a></strong> Synthetic Monocular Depth <br>
<strong> Description: </strong> &ldquo; Synthetic Monocular Depth generation code,&rdquo;
<br />
<a href="javascript:toggleBibtex('Anwar2021DefocusMVA')">[BibTeX]</a>
<a href="https://link.springer.com/article/10.1007/s00138-020-01162-6">[PDF]</a>
<a href="../papers/MVA2020_Defocus.pdf">[arXiv]</a>
<a href="../codes/DefocusingImagesCode.zip">[Code]</a>

</p>
<div id="bib_Anwar2021DefocusMVA" class="bibtex noshow">
<pre>
@article{Anwar2021DefocusMVA,
  title={Deblur and deep depth from single defocus image},
  author={Anwar, Saeed and Hayder, Zeeshan and Porikli, Fatih},
  journal={Machine Vision and Applications},
  volume={32},
  number={1},
  pages={1--13},
  year={2021},
  publisher={Springer}
}
</pre>
</div>
</li>


    </ul>
    </div><!-- /.article-wrap -->
    {% if site.disqus_shortname and page.comments %}
      <section id="disqus_thread"></section><!-- /#disqus_thread -->
    {% endif %}
  </article>
</div><!-- /#index -->
<div class="footer-wrap">
  <footer>
    {% include _footer.html %}
  </footer>
</div><!-- /.footer-wrap -->

{% include _scripts.html %}          

</body>
</html>
